from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
import os
import hashlib
import spacy
from dotenv import load_dotenv
import pdfplumber
import docx
import io
import re
import tempfile
from docx2pdf import convert as docx2pdf_convert

from .db import get_conn, init_cache, DB_PATH
from .text_utils import clean_text, normalize_image_paragraphs, decode_escaped_newlines, normalize_exam_like_image
from .ai_service import GeminiService
from .ocr_service import OCRService

# =========================
# 1ï¸âƒ£ ç¯å¢ƒå˜é‡ & åŸºç¡€é…ç½®
# =========================

BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR / ".env"
load_dotenv(dotenv_path=ENV_PATH)

# ä»£ç†ï¼ˆå¦‚ä¸éœ€è¦å¯åˆ é™¤ï¼‰
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

ai_service = GeminiService(api_key=GEMINI_API_KEY)
ocr_service = OCRService()

# =========================
# 2ï¸âƒ£ FastAPI åˆå§‹åŒ–
# =========================

app = FastAPI(title="English Reader API")

from fastapi.staticfiles import StaticFiles
STATIC_DIR = BASE_DIR / "static"
if not STATIC_DIR.exists():
    STATIC_DIR.mkdir()

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# 3ï¸âƒ£ spaCy æ¨¡å‹
# =========================

nlp = spacy.load("en_core_web_sm")

# Initialize Cache
init_cache()

def make_cache_key(sentence: str, word: str) -> str:
    h = hashlib.md5(sentence.strip().lower().encode()).hexdigest()[:8]
    return f"explain:{h}:{word.lower()}"

# =========================
# 5ï¸âƒ£ æ•°æ®æ¨¡å‹
# =========================

class ParseRequest(BaseModel):
    text: str

class ExplainRequest(BaseModel):
    token_id: str
    word: str
    sentence: str

class TranslateRequest(BaseModel):
    text: str

# =========================
# 6ï¸âƒ£ Core Logic: PDF Extraction
# =========================

def extract_words_with_coords(pdf_file):
    """
    æå– PDF æ–‡æœ¬åŠåæ ‡
    """
    full_text = ""
    text_map = []
    pages_meta = []
    
    current_char_idx = 0
    
    for page_idx, page in enumerate(pdf_file.pages):
        pages_meta.append({
            "page_idx": page_idx,
            "width": float(page.width),
            "height": float(page.height)
        })
        
        words = page.extract_words(
            x_tolerance=1, 
            y_tolerance=1, 
            keep_blank_chars=False
        )
        
        if not words: continue
        
        last_bottom = 0
        last_x1 = 0
        
        for i, w in enumerate(words):
            text = w['text']
            # åˆ¤æ–­æ˜¯å¦æ¢è¡Œ
            if i > 0 and (w['top'] - words[i-1]['top']) > 5:
                full_text += "\n"
                current_char_idx += 1
                last_x1 = 0
            
            need_space = True
            if last_x1 == 0: 
                need_space = False
            elif text.startswith('-') or (full_text and full_text[-1] == '-'):
                 need_space = False
            
            if need_space: 
                full_text += " "
                current_char_idx += 1
            
            start = current_char_idx
            end = start + len(text)
            
            text_map.append({
                "start": start,
                "end": end,
                "text": text,
                "bbox": {
                    "x0": float(w['x0']),
                    "top": float(w['top']),
                    "x1": float(w['x1']),
                    "bottom": float(w['bottom'])
                },
                "page": page_idx,
                "page_width": float(page.width),
                "page_height": float(page.height)
            })
            
            full_text += text
            current_char_idx = end
            last_x1 = w['x1']
            
        full_text += "\n\n"
        current_char_idx += 2
        
    return full_text, text_map, pages_meta

def parse_pdf(file_bytes: bytes):
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        text, text_map, pages_meta = extract_words_with_coords(pdf)
        if len(text.strip()) > 50:
            return clean_text(text), text_map, pages_meta
        
        # Fallback
        text = ""
        for page in pdf.pages:
            t = page.extract_text(x_tolerance=2, y_tolerance=2)
            if t: text += t + "\n"
        return clean_text(text), None, []

def parse_docx(file_bytes: bytes) -> str:
    """
    è§£æ Word æ–‡æ¡£ï¼Œå°½é‡ä¿ç•™åŸå§‹æ®µè½å¸ƒå±€ï¼š
    - æ¯ä¸ª Word æ®µè½ä¹‹é—´ä½¿ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦ \\n\\n åˆ†éš”ï¼ˆå¯¹åº”å‰ç«¯ä¸­çš„â€œæ–°æ®µè½â€ï¼‰
    - æ®µè½å†…éƒ¨ä¸åšä»»ä½•æ¢è¡Œåˆå¹¶ï¼Œä¿æŒä¸åŸæ–‡ä¸€è‡´
    - ä¸å†è°ƒç”¨ clean_textï¼Œé¿å…è¯¯åˆ ç©ºè¡Œæˆ–åˆå¹¶æ®µè½
    """
    doc = docx.Document(io.BytesIO(file_bytes))
    paragraphs: list[str] = []

    for para in doc.paragraphs:
        # para.text å·²ç»æŠŠè¯¥æ®µè½å†…çš„ run åˆå¹¶ï¼ŒåŒ…å«æ‰‹åŠ¨æ¢è¡Œç¬¦
        text = para.text
        if text is None:
            continue
        # ä¿ç•™æ®µå†…ç©ºæ ¼ï¼Œåªå»æ‰æ®µå°¾å¤šä½™ç©ºç™½
        paragraphs.append(text.rstrip())

    # ä½¿ç”¨ \\n\\n ä½œä¸ºæ®µè½åˆ†éš”ç¬¦ï¼Œæ–¹ä¾¿åç»­ process_text æŒ‰è‡ªç„¶æ®µåˆ‡åˆ†
    joined = "\n\n".join(p for p in paragraphs)
    return joined.strip()

def process_text(raw_text: str, word_map=None):
    """
    æ–‡æœ¬ç»“æ„åŒ–æ ¸å¿ƒé€»è¾‘
    å…³é”®ç‚¹ï¼š**æ®µè½ä¼˜å…ˆ**â€”â€”å…ˆæŒ‰åŸå§‹æ–‡æœ¬é‡Œçš„è‡ªç„¶æ®µ(\n\n)åˆ‡åˆ†ï¼Œå†å¯¹æ¯ä¸ªæ®µè½å•ç‹¬åšå¥å­ä¸åˆ†è¯ã€‚
    è¿™æ ·å¯ä»¥å®Œå…¨å°Šé‡ OCR / åŸæ–‡ä¸­çš„æ®µè½æ’ç‰ˆï¼Œè€Œä¸ä¼šå› ä¸º spaCy çš„å¥å­åˆ‡åˆ†è€Œä¸¢å¤±æ®µè½ä¿¡æ¯ã€‚
    """
    sentences = []

    # å…ˆæŒ‰ \n\nï¼ˆå…è®¸ä¸­é—´æœ‰ç©ºç™½ï¼‰åˆ‡æˆè‡ªç„¶æ®µï¼Œç¡®ä¿æ®µè½ä¿¡æ¯æ¥è‡ªåŸæ–‡è€Œä¸æ˜¯ spaCy çš„å¥å­åˆ†å‰²
    paragraph_splits = re.split(r"\n\s*\n+", raw_text)
    offset = 0  # ç”¨æ¥æŠŠæ®µè½å†…çš„ç›¸å¯¹ä¸‹æ ‡ï¼Œæ˜ å°„å›æ•´ç¯‡æ–‡ç« ä¸­çš„ç»å¯¹ä¸‹æ ‡

    print(f"DEBUG: Detected {len(paragraph_splits)} paragraphs from raw_text")

    suffixes = ["n't", "'s", "'ll", "'re", "'ve", "'m", "'d"]

    for para_idx, para_text in enumerate(paragraph_splits):
        if not para_text.strip():
            # ç©ºæ®µè½ï¼šç›´æ¥æŠŠ offset æ¨è¿›ï¼ˆ+2 æ¨¡æ‹Ÿä¹‹å‰çš„ \n\nï¼‰
            offset += len(para_text) + 2
            continue

        # å¯¹å½“å‰æ®µè½å•ç‹¬åš NLP å¥å­åˆ†å‰²
        doc = nlp(para_text)
        para_sentences = list(doc.sents)
        print(f"DEBUG: Paragraph {para_idx} -> {len(para_sentences)} sentences")

        for inner_idx, sent in enumerate(para_sentences):
            global_sent_idx = len(sentences)  # ç”¨å…¨å±€åºå·ç”Ÿæˆ token_id

            layout_info = {
                "is_new_paragraph": inner_idx == 0,  # æ¯ä¸ªæ®µè½é‡Œçš„ç¬¬ä¸€å¥æ ‡è®°ä¸ºæ–°æ®µè½
                "indent_level": 0
            }

            spacy_tokens = list(sent)
            merged_tokens = []

            for token in spacy_tokens:
                if token.is_space:
                    continue

                should_merge = False
                if merged_tokens:
                    text_lower = token.text.lower()
                    if text_lower in suffixes:
                        should_merge = True

                if should_merge:
                    prev_token = merged_tokens[-1]
                    prev_token["text"] += token.text
                    prev_token["end"] = offset + token.idx + len(token.text)
                    prev_token["has_space_after"] = bool(token.whitespace_)
                else:
                    global_start = offset + token.idx
                    global_end = global_start + len(token.text)

                    token_data = {
                        "token_id": f"sent-{global_sent_idx}-token-{len(merged_tokens)}",
                        "text": token.text,
                        "lemma": token.lemma_,
                        "pos": token.pos_,
                        "tag": token.tag_,
                        "dep": token.dep_,
                        "start": global_start,
                        "end": global_end,
                        "has_space_after": bool(token.whitespace_),
                    }

                    if word_map:
                        matched_rects = []
                        t_start, t_end = global_start, global_end

                        for wm in word_map:
                            if max(t_start, wm["start"]) < min(t_end, wm["end"]):
                                matched_rects.append(wm)

                        if matched_rects:
                            page_idx = matched_rects[0]["page"]
                            x0 = min(r["bbox"]["x0"] for r in matched_rects)
                            top = min(r["bbox"]["top"] for r in matched_rects)
                            x1 = max(r["bbox"]["x1"] for r in matched_rects)
                            bottom = max(r["bbox"]["bottom"] for r in matched_rects)

                            token_data["bbox"] = {
                                "page": page_idx,
                                "x0": x0,
                                "top": top,
                                "x1": x1,
                                "bottom": bottom,
                                "width": x1 - x0,
                                "height": bottom - top,
                            }

                    merged_tokens.append(token_data)

            if merged_tokens:
                sentences.append(
                    {
                        "text": sent.text,
                        "start": offset + sent.start_char,
                        "end": offset + sent.end_char,
                        "layout": layout_info,
                        "tokens": merged_tokens,
                    }
                )

        # æœ¬æ®µåœ¨åŸæ–‡ä¸­å ç”¨çš„é•¿åº¦ï¼šæ®µè½å†…å®¹ + ä¹‹å‰çš„ \n\nï¼ˆè¿™é‡Œç®€åŒ–ç”¨ +2ï¼‰
        offset += len(para_text) + 2

    return {"sentences": sentences}


# =========================
# 7ï¸âƒ£ API Routes
# =========================

@app.post("/upload-file")
async def upload_file(file: UploadFile = File(...)):
    content = await file.read()
    filename = file.filename.lower()
    
    text = ""
    word_map = None
    pages_meta = []
    file_url = ""
    source_type = "other"
    docx_image_ocr_texts = []  # ç”¨äºå­˜å‚¨ Word æ–‡æ¡£ä¸­å›¾ç‰‡çš„ OCR ç»“æœ
    
    try:
        if filename.endswith(".pdf"):
            safe_name = f"{hashlib.md5(content).hexdigest()[:10]}.pdf"
            save_path = STATIC_DIR / "uploads" / safe_name
            
            # Ensure uploads directory exists
            (STATIC_DIR / "uploads").mkdir(exist_ok=True)
            
            with open(save_path, "wb") as f:
                f.write(content)
            file_url = f"http://127.0.0.1:8000/static/uploads/{safe_name}"

            text, word_map, pages_meta = parse_pdf(content)
            source_type = "pdf"
        elif filename.endswith(".docx"):
            # âœ¨ æ–°æ–¹æ¡ˆï¼šå°† Word è½¬æ¢ä¸º PDFï¼Œå¤ç”¨ PDF æ¸²æŸ“é€»è¾‘ä»¥ä¿æŒå®Œç¾æ ¼å¼
            # åŒæ—¶æå– Word ä¸­çš„å›¾ç‰‡è¿›è¡Œ OCRï¼Œè®©å›¾ç‰‡ä¸­çš„æ–‡å­—ä¹Ÿå¯ä»¥ç‚¹å‡»æŸ¥è¯
            print("DEBUG: Converting Word document to PDF for native rendering...")
            
            # Ensure uploads directory exists
            (STATIC_DIR / "uploads").mkdir(exist_ok=True)
            
            # ç”¨äºå­˜å‚¨ Word ä¸­å›¾ç‰‡çš„ OCR ç»“æœ
            docx_image_ocr_texts = []
            
            # 1. å°† docx ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as tmp_dir:
                docx_path = Path(tmp_dir) / "input.docx"
                pdf_path = Path(tmp_dir) / "input.pdf"
                
                with open(docx_path, "wb") as f:
                    f.write(content)
                
                # 1.5 æå– Word ä¸­çš„æ‰€æœ‰å›¾ç‰‡å¹¶è¿›è¡Œ OCR
                try:
                    doc = docx.Document(io.BytesIO(content))
                    image_count = 0
                    
                    # éå†æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…³ç³»ï¼Œæ‰¾åˆ°å›¾ç‰‡
                    for rel in doc.part.rels.values():
                        if "image" in rel.target_ref:
                            try:
                                image_data = rel.target_part.blob
                                image_count += 1
                                print(f"DEBUG: Found embedded image #{image_count}, size: {len(image_data)} bytes")
                                
                                # å¯¹å›¾ç‰‡è¿›è¡Œ OCR
                                ocr_text = ocr_service.parse_image(image_data)
                                if ocr_text and ocr_text.strip():
                                    # ä¸ç›´æ¥ä¸Šä¼ å›¾ç‰‡ä¸€è‡´ï¼šåº”ç”¨ clean_text å’Œ normalize_exam_like_image å¤„ç†
                                    processed_ocr = clean_text(ocr_text)
                                    processed_ocr = normalize_exam_like_image(processed_ocr)
                                    
                                    docx_image_ocr_texts.append({
                                        "image_index": image_count,
                                        "ocr_text": processed_ocr.strip()
                                    })
                                    print(f"DEBUG: OCR result for image #{image_count}: {processed_ocr[:100]}...")
                            except Exception as img_err:
                                print(f"WARNING: Failed to OCR image #{image_count}: {img_err}")
                    
                    print(f"DEBUG: Extracted and OCR'd {len(docx_image_ocr_texts)} images from Word document")
                except Exception as extract_err:
                    print(f"WARNING: Image extraction failed: {extract_err}")
                
                # 2. ä½¿ç”¨ docx2pdf è½¬æ¢ï¼ˆéœ€è¦ Microsoft Wordï¼‰
                try:
                    docx2pdf_convert(str(docx_path), str(pdf_path))
                except Exception as convert_err:
                    print(f"ERROR: docx2pdf conversion failed: {convert_err}")
                    raise HTTPException(
                        status_code=500, 
                        detail=f"Word è½¬ PDF å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²å®‰è£… Microsoft Word: {str(convert_err)}"
                    )
                
                # 3. è¯»å–ç”Ÿæˆçš„ PDF
                with open(pdf_path, "rb") as f:
                    pdf_bytes = f.read()
                
                # 4. ä¿å­˜ PDF åˆ° static ç›®å½•ä¾›å‰ç«¯è®¿é—®
                safe_name = f"{hashlib.md5(content).hexdigest()[:10]}.pdf"
                save_path = STATIC_DIR / "uploads" / safe_name
                with open(save_path, "wb") as f:
                    f.write(pdf_bytes)
                file_url = f"http://127.0.0.1:8000/static/uploads/{safe_name}"
                
                # 5. ä½¿ç”¨ PDF è§£æé€»è¾‘æå–æ–‡æœ¬å’Œåæ ‡
                text, word_map, pages_meta = parse_pdf(pdf_bytes)
                source_type = "docx"  # ğŸ‘ˆ æ ‡è®°ä¸º docxï¼Œå‰ç«¯å¯åŒºåˆ†å¤„ç†
                
            print(f"DEBUG: Word->PDF conversion successful, pages: {len(pages_meta)}, OCR images: {len(docx_image_ocr_texts)}")
        elif filename.endswith((".jpg", ".jpeg", ".png", ".webp")):
            # Use local OCR Service (PaddleOCR)
            text = ocr_service.parse_image(content)
            source_type = "image"
        elif filename.endswith(".txt"):
            text = content.decode("utf-8")
            source_type = "txt"
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            
        if not text.strip():
             raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯†åˆ«")
        
        if word_map:
            final_text = text
        else:
            # Process based on file type
            if filename.endswith((".jpg", ".jpeg", ".png", ".webp")):
                # å›¾ç‰‡ OCRï¼šå…ˆåšé€šç”¨æ¸…ç†ï¼Œå†å¯¹â€œè¯•å·ç±»â€ç»“æ„åšè½»é‡æ’ç‰ˆä¿®æ­£
                # 1) clean_text: åˆå¹¶æ®µå†…ç¡¬å›è½¦ï¼Œåªä¿ç•™çœŸæ­£æ®µè½
                final_text = clean_text(text)
                # 2) å¦‚æœæ£€æµ‹åˆ° 17.A)ã€19.B) è¿™ç±»é¢˜å·ç»“æ„ï¼Œå†åšä¸“é—¨çš„æ’ç‰ˆä¼˜åŒ–
                final_text = normalize_exam_like_image(final_text)
                # DEBUG: Check if paragraph breaks are present after cleaning
                para_count = final_text.count('\n\n')
                line_count = final_text.count('\n') - para_count * 2
                print(f"DEBUG: Image OCR text - {para_count} paragraphs, {line_count} line breaks")
                print(f"DEBUG: Text preview: {final_text[:300]}...")
            elif filename.endswith(".docx"):
                # Word ç°åœ¨å·²ç»è½¬æ¢ä¸º PDFï¼Œä¼šèµ° word_map åˆ†æ”¯ï¼Œè¿™é‡Œæ˜¯å…œåº•é€»è¾‘
                final_text = text
            else:
                # å…¶å®ƒçº¯æ–‡æœ¬ç±»ï¼ˆtxt ç­‰ï¼‰ï¼Œåšä¸€æ¬¡åŸºç¡€æ¸…ç†
                final_text = clean_text(text)
            
        result = process_text(final_text, word_map=word_map)
        
        # é™„åŠ åŸå§‹æ–‡æœ¬ä¸å…ƒä¿¡æ¯ï¼Œå‰ç«¯å¯é€‰æ‹©ç›´æ¥æŒ‰åŸå§‹æ¢è¡Œæ¸²æŸ“
        result["raw_text"] = final_text
        result["file_url"] = file_url
        result["pages"] = pages_meta
        result["source_type"] = source_type
        
        # å¦‚æœæœ‰ Word æ–‡æ¡£ä¸­å›¾ç‰‡çš„ OCR ç»“æœï¼Œä¹Ÿä¸€å¹¶è¿”å›
        if docx_image_ocr_texts:
            result["docx_image_ocr"] = docx_image_ocr_texts
            # åˆå¹¶æ‰€æœ‰å›¾ç‰‡ OCR æ–‡æœ¬ï¼Œæ–¹ä¾¿å‰ç«¯æ¸²æŸ“
            combined_ocr = "\n\n".join([item["ocr_text"] for item in docx_image_ocr_texts])
            result["docx_image_ocr_combined"] = combined_ocr
            print(f"DEBUG: Returning {len(docx_image_ocr_texts)} image OCR results")
        
        return result

    except Exception as e:
        if isinstance(e, HTTPException):
            raise e
        print(f"è§£æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")

@app.post("/parse-text")
def parse_text(req: ParseRequest):
    return process_text(req.text)

@app.post("/explain-token")
def explain_token(req: ExplainRequest):
    cache_key = make_cache_key(req.sentence, req.word)

    conn = get_conn()
    cur = conn.cursor()

    # 1. Check Cache
    cur.execute(
        "SELECT meaning_zh, explanation_zh FROM explain_cache WHERE cache_key = ?",
        (cache_key,)
    )
    row = cur.fetchone()

    if row:
        conn.close()
        return {
            "word": req.word,
            "meaning_zh": row[0],
            "explanation_zh": row[1],
            "confidence": 0.95
        }

    # 2. Call AI
    meaning, explanation = ai_service.explain_word(req.word, req.sentence)

    # 3. Write Cache
    cur.execute(
        """
        INSERT INTO explain_cache
        (cache_key, word, sentence, meaning_zh, explanation_zh)
        VALUES (?, ?, ?, ?, ?)
        """,
        (cache_key, req.word, req.sentence, meaning, explanation)
    )
    conn.commit()
    conn.close()

    return {
        "word": req.word,
        "meaning_zh": meaning,
        "explanation_zh": explanation,
        "confidence": 0.95
    }

@app.post("/translate-text")
def translate_text(req: TranslateRequest):
    translation = ai_service.translate_text(req.text)
    return {
        "translation_zh": translation
    }
