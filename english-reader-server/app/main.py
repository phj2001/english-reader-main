from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
import os
import hashlib
import spacy
from dotenv import load_dotenv
import pdfplumber
import docx
import io
import re
import tempfile
from docx2pdf import convert as docx2pdf_convert

from .db import get_conn, init_cache, DB_PATH
from .text_utils import clean_text, decode_escaped_newlines, normalize_exam_like_image
from .ai_service import AIService, GeminiService
from .ocr_service import OCRService
from .config_manager import config_manager

# =========================
# 1ï¸âƒ£ ç¯å¢ƒå˜é‡ & åŸºç¡€é…ç½®
# =========================

BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR / ".env"
load_dotenv(dotenv_path=ENV_PATH)

# ä»£ç†é…ç½®ï¼ˆå¦‚ä¸éœ€è¦å¯åœ¨ .env ä¸­è®¾ç½® USE_PROXY=falseï¼‰
if os.getenv("USE_PROXY", "true").lower() == "true":
    os.environ["HTTP_PROXY"] = os.getenv("HTTP_PROXY", "http://127.0.0.1:7897")
    os.environ["HTTPS_PROXY"] = os.getenv("HTTPS_PROXY", "http://127.0.0.1:7897")

# ============================================
# âœï¸ AI æ¨¡å‹é…ç½® - åœ¨ .env æ–‡ä»¶ä¸­ä¿®æ”¹
# ============================================
# 
# AI_PROVIDER: é€‰æ‹©ä½¿ç”¨çš„ AI æœåŠ¡æä¾›å•†
#   - "gemini"  : Google Gemini (ä½¿ç”¨åŸç”Ÿ SDK)
#   - "openai"  : OpenAI å…¼å®¹ API (è±†åŒ…ã€é€šä¹‰ã€DeepSeek ç­‰)
#
# å¦‚æœä½¿ç”¨ Gemini:
#   GEMINI_API_KEY=ä½ çš„APIå¯†é’¥
#   GEMINI_MODEL_NAME=gemini-1.5-flash
#
# å¦‚æœä½¿ç”¨ OpenAI å…¼å®¹ API (è±†åŒ…/é€šä¹‰/DeepSeek ç­‰):
#   AI_API_KEY=ä½ çš„APIå¯†é’¥
#   AI_BASE_URL=https://api.example.com/v1
#   AI_MODEL_NAME=æ¨¡å‹åç§°
#
AI_PROVIDER = os.getenv("AI_PROVIDER", "gemini")

if AI_PROVIDER == "gemini":
    # ä½¿ç”¨ Google Gemini
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME")
    ai_service = GeminiService(api_key=GEMINI_API_KEY, model_name=GEMINI_MODEL_NAME)
else:
    # ä½¿ç”¨ OpenAI å…¼å®¹ API (è±†åŒ…ã€é€šä¹‰ã€DeepSeekã€OpenAI ç­‰)
    AI_API_KEY = os.getenv("AI_API_KEY")
    AI_BASE_URL = os.getenv("AI_BASE_URL", "https://api.openai.com/v1")
    AI_MODEL_NAME = os.getenv("AI_MODEL_NAME")
    ai_service = AIService(api_key=AI_API_KEY, base_url=AI_BASE_URL, model_name=AI_MODEL_NAME)

ocr_service = OCRService()

# =========================
# 2ï¸âƒ£ FastAPI åˆå§‹åŒ–
# =========================

app = FastAPI(title="English Reader API")

from fastapi.staticfiles import StaticFiles
STATIC_DIR = BASE_DIR / "static"
if not STATIC_DIR.exists():
    STATIC_DIR.mkdir()

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# 3ï¸âƒ£ spaCy æ¨¡å‹
# =========================

nlp = spacy.load("en_core_web_sm")

# Initialize Cache
init_cache()

def make_cache_key(sentence: str, word: str, ai_config: str = "") -> str:
    """
    ç”Ÿæˆç¼“å­˜ key

    Args:
        sentence: å¥å­æ–‡æœ¬
        word: å•è¯
        ai_config: AI é…ç½®æ ‡è¯†ï¼ˆprovider + modelï¼‰
    """
    sentence_hash = hashlib.md5(sentence.strip().lower().encode()).hexdigest()[:8]
    word_lower = word.lower()

    # å¦‚æœæä¾›äº† AI é…ç½®ï¼ŒåŠ å…¥ç¼“å­˜ key
    if ai_config:
        return f"explain:{sentence_hash}:{word_lower}:{ai_config}"
    else:
        # å…¼å®¹æ—§çš„ç¼“å­˜ keyï¼ˆé»˜è®¤é…ç½®ï¼‰
        return f"explain:{sentence_hash}:{word_lower}"


def make_ai_config_key(provider: str = None, model: str = None) -> str:
    """ç”Ÿæˆ AI é…ç½®æ ‡è¯†ç¬¦"""
    if not provider:
        return "default"
    return f"{provider}:{model or 'default'}"


def create_ai_service(ai_provider: str = None, ai_api_key: str = None,
                      ai_base_url: str = None, ai_model_name: str = None,
                      gemini_api_key: str = None, gemini_model_name: str = None):
    """
    åˆ›å»º AI æœåŠ¡å®ä¾‹ï¼ˆæ”¯æŒåŠ¨æ€é…ç½®ï¼‰

    å¦‚æœæä¾›äº†åŠ¨æ€é…ç½®å‚æ•°ï¼Œä½¿ç”¨åŠ¨æ€é…ç½®åˆ›å»ºä¸´æ—¶å®ä¾‹
    å¦åˆ™ä½¿ç”¨å…¨å±€çš„ ai_serviceï¼ˆä» .env åŠ è½½çš„é»˜è®¤é…ç½®ï¼‰
    """
    # å¦‚æœæ²¡æœ‰æä¾›åŠ¨æ€é…ç½®ï¼Œä½¿ç”¨å…¨å±€é»˜è®¤æœåŠ¡
    if not ai_provider:
        return ai_service

    # å¦‚æœæä¾›äº†åŠ¨æ€é…ç½®ï¼Œåˆ›å»ºä¸´æ—¶æœåŠ¡å®ä¾‹
    try:
        if ai_provider == "gemini":
            if not gemini_api_key:
                raise ValueError("Missing Gemini API Key")
            if not gemini_model_name:
                gemini_model_name = "gemini-1.5-flash"
            return GeminiService(api_key=gemini_api_key, model_name=gemini_model_name)
        else:
            if not ai_api_key:
                raise ValueError("Missing API Key")
            if not ai_base_url:
                ai_base_url = "https://api.openai.com/v1"
            if not ai_model_name:
                raise ValueError("Missing model name")
            return AIService(api_key=ai_api_key, base_url=ai_base_url, model_name=ai_model_name)
    except Exception as e:
        print(f"Error creating AI service: {e}")
        # å¦‚æœåŠ¨æ€é…ç½®å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æœåŠ¡
        return ai_service

# =========================
# 5ï¸âƒ£ æ•°æ®æ¨¡å‹
# =========================

class ParseRequest(BaseModel):
    text: str

class ExplainRequest(BaseModel):
    token_id: str
    word: str
    sentence: str
    # å¯é€‰çš„åŠ¨æ€ AI é…ç½®
    ai_provider: str = None
    ai_api_key: str = None
    ai_base_url: str = None
    ai_model_name: str = None
    gemini_api_key: str = None
    gemini_model_name: str = None

class TranslateRequest(BaseModel):
    text: str
    # å¯é€‰çš„åŠ¨æ€ AI é…ç½®
    ai_provider: str = None
    ai_api_key: str = None
    ai_base_url: str = None
    ai_model_name: str = None
    gemini_api_key: str = None
    gemini_model_name: str = None

# =========================
# 6ï¸âƒ£ Core Logic: PDF Extraction
# =========================

def extract_words_with_coords(pdf_file):
    """
    æå– PDF æ–‡æœ¬åŠåæ ‡
    """
    full_text = ""
    text_map = []
    pages_meta = []
    
    current_char_idx = 0
    
    for page_idx, page in enumerate(pdf_file.pages):
        pages_meta.append({
            "page_idx": page_idx,
            "width": float(page.width),
            "height": float(page.height)
        })
        
        words = page.extract_words(
            x_tolerance=1, 
            y_tolerance=1, 
            keep_blank_chars=False
        )
        
        if not words: continue
        
        last_bottom = 0
        last_x1 = 0
        
        for i, w in enumerate(words):
            text = w['text']
            # åˆ¤æ–­æ˜¯å¦æ¢è¡Œ
            if i > 0 and (w['top'] - words[i-1]['top']) > 5:
                full_text += "\n"
                current_char_idx += 1
                last_x1 = 0
            
            need_space = True
            if last_x1 == 0: 
                need_space = False
            elif text.startswith('-') or (full_text and full_text[-1] == '-'):
                 need_space = False
            
            if need_space: 
                full_text += " "
                current_char_idx += 1
            
            start = current_char_idx
            end = start + len(text)
            
            text_map.append({
                "start": start,
                "end": end,
                "text": text,
                "bbox": {
                    "x0": float(w['x0']),
                    "top": float(w['top']),
                    "x1": float(w['x1']),
                    "bottom": float(w['bottom'])
                },
                "page": page_idx,
                "page_width": float(page.width),
                "page_height": float(page.height)
            })
            
            full_text += text
            current_char_idx = end
            last_x1 = w['x1']
            
        full_text += "\n\n"
        current_char_idx += 2
        
    return full_text, text_map, pages_meta

def parse_pdf(file_bytes: bytes):
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        text, text_map, pages_meta = extract_words_with_coords(pdf)
        if len(text.strip()) > 50:
            return clean_text(text), text_map, pages_meta
        
        # Fallback
        text = ""
        for page in pdf.pages:
            t = page.extract_text(x_tolerance=2, y_tolerance=2)
            if t: text += t + "\n"
        return clean_text(text), None, []


def process_text(raw_text: str, word_map=None):
    """
    æ–‡æœ¬ç»“æ„åŒ–æ ¸å¿ƒé€»è¾‘
    å…³é”®ç‚¹ï¼š**æ®µè½ä¼˜å…ˆ**â€”â€”å…ˆæŒ‰åŸå§‹æ–‡æœ¬é‡Œçš„è‡ªç„¶æ®µ(\n\n)åˆ‡åˆ†ï¼Œå†å¯¹æ¯ä¸ªæ®µè½å•ç‹¬åšå¥å­ä¸åˆ†è¯ã€‚
    è¿™æ ·å¯ä»¥å®Œå…¨å°Šé‡ OCR / åŸæ–‡ä¸­çš„æ®µè½æ’ç‰ˆï¼Œè€Œä¸ä¼šå› ä¸º spaCy çš„å¥å­åˆ‡åˆ†è€Œä¸¢å¤±æ®µè½ä¿¡æ¯ã€‚
    """
    sentences = []

    # å…ˆæŒ‰ \n\nï¼ˆå…è®¸ä¸­é—´æœ‰ç©ºç™½ï¼‰åˆ‡æˆè‡ªç„¶æ®µï¼Œç¡®ä¿æ®µè½ä¿¡æ¯æ¥è‡ªåŸæ–‡è€Œä¸æ˜¯ spaCy çš„å¥å­åˆ†å‰²
    paragraph_splits = re.split(r"\n\s*\n+", raw_text)
    offset = 0  # ç”¨æ¥æŠŠæ®µè½å†…çš„ç›¸å¯¹ä¸‹æ ‡ï¼Œæ˜ å°„å›æ•´ç¯‡æ–‡ç« ä¸­çš„ç»å¯¹ä¸‹æ ‡

    print(f"DEBUG: Detected {len(paragraph_splits)} paragraphs from raw_text")

    suffixes = ["n't", "'s", "'ll", "'re", "'ve", "'m", "'d"]

    for para_idx, para_text in enumerate(paragraph_splits):
        if not para_text.strip():
            # ç©ºæ®µè½ï¼šç›´æ¥æŠŠ offset æ¨è¿›ï¼ˆ+2 æ¨¡æ‹Ÿä¹‹å‰çš„ \n\nï¼‰
            offset += len(para_text) + 2
            continue

        # å¯¹å½“å‰æ®µè½å•ç‹¬åš NLP å¥å­åˆ†å‰²
        doc = nlp(para_text)
        para_sentences = list(doc.sents)
        print(f"DEBUG: Paragraph {para_idx} -> {len(para_sentences)} sentences")

        for inner_idx, sent in enumerate(para_sentences):
            global_sent_idx = len(sentences)  # ç”¨å…¨å±€åºå·ç”Ÿæˆ token_id

            layout_info = {
                "is_new_paragraph": inner_idx == 0,  # æ¯ä¸ªæ®µè½é‡Œçš„ç¬¬ä¸€å¥æ ‡è®°ä¸ºæ–°æ®µè½
                "indent_level": 0
            }

            spacy_tokens = list(sent)
            merged_tokens = []

            for token in spacy_tokens:
                if token.is_space:
                    continue

                should_merge = False
                if merged_tokens:
                    text_lower = token.text.lower()
                    if text_lower in suffixes:
                        should_merge = True

                if should_merge:
                    prev_token = merged_tokens[-1]
                    prev_token["text"] += token.text
                    prev_token["end"] = offset + token.idx + len(token.text)
                    prev_token["has_space_after"] = bool(token.whitespace_)
                else:
                    global_start = offset + token.idx
                    global_end = global_start + len(token.text)

                    token_data = {
                        "token_id": f"sent-{global_sent_idx}-token-{len(merged_tokens)}",
                        "text": token.text,
                        "lemma": token.lemma_,
                        "pos": token.pos_,
                        "tag": token.tag_,
                        "dep": token.dep_,
                        "start": global_start,
                        "end": global_end,
                        "has_space_after": bool(token.whitespace_),
                    }

                    if word_map:
                        matched_rects = []
                        t_start, t_end = global_start, global_end

                        for wm in word_map:
                            if max(t_start, wm["start"]) < min(t_end, wm["end"]):
                                matched_rects.append(wm)

                        if matched_rects:
                            page_idx = matched_rects[0]["page"]
                            x0 = min(r["bbox"]["x0"] for r in matched_rects)
                            top = min(r["bbox"]["top"] for r in matched_rects)
                            x1 = max(r["bbox"]["x1"] for r in matched_rects)
                            bottom = max(r["bbox"]["bottom"] for r in matched_rects)

                            token_data["bbox"] = {
                                "page": page_idx,
                                "x0": x0,
                                "top": top,
                                "x1": x1,
                                "bottom": bottom,
                                "width": x1 - x0,
                                "height": bottom - top,
                            }

                    merged_tokens.append(token_data)

            if merged_tokens:
                sentences.append(
                    {
                        "text": sent.text,
                        "start": offset + sent.start_char,
                        "end": offset + sent.end_char,
                        "layout": layout_info,
                        "tokens": merged_tokens,
                    }
                )

        # æœ¬æ®µåœ¨åŸæ–‡ä¸­å ç”¨çš„é•¿åº¦ï¼šæ®µè½å†…å®¹ + ä¹‹å‰çš„ \n\nï¼ˆè¿™é‡Œç®€åŒ–ç”¨ +2ï¼‰
        offset += len(para_text) + 2

    return {"sentences": sentences}


# =========================
# 7ï¸âƒ£ API Routes
# =========================

@app.post("/upload-file")
async def upload_file(file: UploadFile = File(...)):
    content = await file.read()
    filename = file.filename.lower()
    
    text = ""
    word_map = None
    pages_meta = []
    file_url = ""
    source_type = "other"
    docx_image_ocr_texts = []  # ç”¨äºå­˜å‚¨ Word æ–‡æ¡£ä¸­å›¾ç‰‡çš„ OCR ç»“æœ
    
    try:
        if filename.endswith(".pdf"):
            safe_name = f"{hashlib.md5(content).hexdigest()[:10]}.pdf"
            save_path = STATIC_DIR / "uploads" / safe_name
            
            # Ensure uploads directory exists
            (STATIC_DIR / "uploads").mkdir(exist_ok=True)
            
            with open(save_path, "wb") as f:
                f.write(content)
            file_url = f"http://127.0.0.1:8000/static/uploads/{safe_name}"

            text, word_map, pages_meta = parse_pdf(content)
            source_type = "pdf"
        elif filename.endswith(".docx"):
            # âœ¨ æ–°æ–¹æ¡ˆï¼šå°† Word è½¬æ¢ä¸º PDFï¼Œå¤ç”¨ PDF æ¸²æŸ“é€»è¾‘ä»¥ä¿æŒå®Œç¾æ ¼å¼
            # åŒæ—¶æå– Word ä¸­çš„å›¾ç‰‡è¿›è¡Œ OCRï¼Œè®©å›¾ç‰‡ä¸­çš„æ–‡å­—ä¹Ÿå¯ä»¥ç‚¹å‡»æŸ¥è¯
            print("DEBUG: Converting Word document to PDF for native rendering...")
            
            # Ensure uploads directory exists
            (STATIC_DIR / "uploads").mkdir(exist_ok=True)
            
            # ç”¨äºå­˜å‚¨ Word ä¸­å›¾ç‰‡çš„ OCR ç»“æœ
            docx_image_ocr_texts = []
            
            # 1. å°† docx ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as tmp_dir:
                docx_path = Path(tmp_dir) / "input.docx"
                pdf_path = Path(tmp_dir) / "input.pdf"
                
                with open(docx_path, "wb") as f:
                    f.write(content)
                
                # 1.5 æå– Word ä¸­çš„æ‰€æœ‰å›¾ç‰‡å¹¶è¿›è¡Œ OCR
                try:
                    doc = docx.Document(io.BytesIO(content))
                    image_count = 0
                    
                    # éå†æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…³ç³»ï¼Œæ‰¾åˆ°å›¾ç‰‡
                    for rel in doc.part.rels.values():
                        if "image" in rel.target_ref:
                            try:
                                image_data = rel.target_part.blob
                                image_count += 1
                                print(f"DEBUG: Found embedded image #{image_count}, size: {len(image_data)} bytes")
                                
                                # å¯¹å›¾ç‰‡è¿›è¡Œ OCR
                                ocr_text = ocr_service.parse_image(image_data)
                                if ocr_text and ocr_text.strip():
                                    # ä¸ç›´æ¥ä¸Šä¼ å›¾ç‰‡ä¸€è‡´ï¼šåº”ç”¨ clean_text å’Œ normalize_exam_like_image å¤„ç†
                                    processed_ocr = clean_text(ocr_text)
                                    processed_ocr = normalize_exam_like_image(processed_ocr)
                                    
                                    docx_image_ocr_texts.append({
                                        "image_index": image_count,
                                        "ocr_text": processed_ocr.strip()
                                    })
                                    print(f"DEBUG: OCR result for image #{image_count}: {processed_ocr[:100]}...")
                            except Exception as img_err:
                                print(f"WARNING: Failed to OCR image #{image_count}: {img_err}")
                    
                    print(f"DEBUG: Extracted and OCR'd {len(docx_image_ocr_texts)} images from Word document")
                except Exception as extract_err:
                    print(f"WARNING: Image extraction failed: {extract_err}")
                
                # 2. ä½¿ç”¨ docx2pdf è½¬æ¢ï¼ˆéœ€è¦ Microsoft Wordï¼‰
                try:
                    docx2pdf_convert(str(docx_path), str(pdf_path))
                except Exception as convert_err:
                    print(f"ERROR: docx2pdf conversion failed: {convert_err}")
                    raise HTTPException(
                        status_code=500, 
                        detail=f"Word è½¬ PDF å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²å®‰è£… Microsoft Word: {str(convert_err)}"
                    )
                
                # 3. è¯»å–ç”Ÿæˆçš„ PDF
                with open(pdf_path, "rb") as f:
                    pdf_bytes = f.read()
                
                # 4. ä¿å­˜ PDF åˆ° static ç›®å½•ä¾›å‰ç«¯è®¿é—®
                safe_name = f"{hashlib.md5(content).hexdigest()[:10]}.pdf"
                save_path = STATIC_DIR / "uploads" / safe_name
                with open(save_path, "wb") as f:
                    f.write(pdf_bytes)
                file_url = f"http://127.0.0.1:8000/static/uploads/{safe_name}"
                
                # 5. ä½¿ç”¨ PDF è§£æé€»è¾‘æå–æ–‡æœ¬å’Œåæ ‡
                text, word_map, pages_meta = parse_pdf(pdf_bytes)
                source_type = "docx"  # ğŸ‘ˆ æ ‡è®°ä¸º docxï¼Œå‰ç«¯å¯åŒºåˆ†å¤„ç†
                
            print(f"DEBUG: Word->PDF conversion successful, pages: {len(pages_meta)}, OCR images: {len(docx_image_ocr_texts)}")
        elif filename.endswith((".jpg", ".jpeg", ".png", ".webp")):
            # Use local OCR Service (PaddleOCR)
            text = ocr_service.parse_image(content)
            source_type = "image"
        elif filename.endswith(".txt"):
            text = content.decode("utf-8")
            source_type = "txt"
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            
        if not text.strip():
             raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯†åˆ«")
        
        if word_map:
            final_text = text
        else:
            # Process based on file type
            if filename.endswith((".jpg", ".jpeg", ".png", ".webp")):
                # å›¾ç‰‡ OCRï¼šå…ˆåšé€šç”¨æ¸…ç†ï¼Œå†å¯¹â€œè¯•å·ç±»â€ç»“æ„åšè½»é‡æ’ç‰ˆä¿®æ­£
                # 1) clean_text: åˆå¹¶æ®µå†…ç¡¬å›è½¦ï¼Œåªä¿ç•™çœŸæ­£æ®µè½
                final_text = clean_text(text)
                # 2) å¦‚æœæ£€æµ‹åˆ° 17.A)ã€19.B) è¿™ç±»é¢˜å·ç»“æ„ï¼Œå†åšä¸“é—¨çš„æ’ç‰ˆä¼˜åŒ–
                final_text = normalize_exam_like_image(final_text)
                # DEBUG: Check if paragraph breaks are present after cleaning
                para_count = final_text.count('\n\n')
                line_count = final_text.count('\n') - para_count * 2
                print(f"DEBUG: Image OCR text - {para_count} paragraphs, {line_count} line breaks")
                print(f"DEBUG: Text preview: {final_text[:300]}...")
            elif filename.endswith(".docx"):
                # Word ç°åœ¨å·²ç»è½¬æ¢ä¸º PDFï¼Œä¼šèµ° word_map åˆ†æ”¯ï¼Œè¿™é‡Œæ˜¯å…œåº•é€»è¾‘
                final_text = text
            else:
                # å…¶å®ƒçº¯æ–‡æœ¬ç±»ï¼ˆtxt ç­‰ï¼‰ï¼Œåšä¸€æ¬¡åŸºç¡€æ¸…ç†
                final_text = clean_text(text)
            
        result = process_text(final_text, word_map=word_map)
        
        # é™„åŠ åŸå§‹æ–‡æœ¬ä¸å…ƒä¿¡æ¯ï¼Œå‰ç«¯å¯é€‰æ‹©ç›´æ¥æŒ‰åŸå§‹æ¢è¡Œæ¸²æŸ“
        result["raw_text"] = final_text
        result["file_url"] = file_url
        result["pages"] = pages_meta
        result["source_type"] = source_type
        
        # å¦‚æœæœ‰ Word æ–‡æ¡£ä¸­å›¾ç‰‡çš„ OCR ç»“æœï¼Œä¹Ÿä¸€å¹¶è¿”å›
        if docx_image_ocr_texts:
            result["docx_image_ocr"] = docx_image_ocr_texts
            # åˆå¹¶æ‰€æœ‰å›¾ç‰‡ OCR æ–‡æœ¬ï¼Œæ–¹ä¾¿å‰ç«¯æ¸²æŸ“
            combined_ocr = "\n\n".join([item["ocr_text"] for item in docx_image_ocr_texts])
            result["docx_image_ocr_combined"] = combined_ocr
            print(f"DEBUG: Returning {len(docx_image_ocr_texts)} image OCR results")
        
        return result

    except Exception as e:
        if isinstance(e, HTTPException):
            raise e
        print(f"è§£æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")

@app.post("/parse-text")
def parse_text(req: ParseRequest):
    return process_text(req.text)

@app.post("/explain-token")
def explain_token(req: ExplainRequest):
    # 1. ç”Ÿæˆ AI é…ç½®æ ‡è¯†ï¼ˆç”¨äºç¼“å­˜ keyï¼‰
    if req.ai_provider == "gemini":
        ai_config_key = make_ai_config_key("gemini", req.gemini_model_name)
    elif req.ai_provider:
        ai_config_key = make_ai_config_key(req.ai_provider, req.ai_model_name)
    else:
        ai_config_key = ""

    # 2. ç”Ÿæˆç¼“å­˜ keyï¼ˆåŒ…å« AI é…ç½®ï¼‰
    cache_key = make_cache_key(req.sentence, req.word, ai_config_key)

    conn = get_conn()
    cur = conn.cursor()

    # 3. Check Cacheï¼ˆå…ˆæŸ¥ç¼“å­˜ï¼Œå‘½ä¸­åˆ™ç›´æ¥è¿”å›ï¼‰
    cur.execute(
        "SELECT meaning_zh, explanation_zh FROM explain_cache WHERE cache_key = ?",
        (cache_key,)
    )
    row = cur.fetchone()

    if row:
        conn.close()
        print(f"[CACHE HIT] {req.word} - {ai_config_key}")  # è°ƒè¯•æ—¥å¿—
        return {
            "word": req.word,
            "meaning_zh": row[0],
            "explanation_zh": row[1],
            "confidence": 0.95
        }

    print(f"[CACHE MISS] {req.word} - {ai_config_key}")  # è°ƒè¯•æ—¥å¿—

    # 4. ç¼“å­˜æœªå‘½ä¸­ï¼Œåˆ›å»º AI æœåŠ¡å¹¶è°ƒç”¨
    service = create_ai_service(
        ai_provider=req.ai_provider,
        ai_api_key=req.ai_api_key,
        ai_base_url=req.ai_base_url,
        ai_model_name=req.ai_model_name,
        gemini_api_key=req.gemini_api_key,
        gemini_model_name=req.gemini_model_name
    )

    meaning, explanation = service.explain_word(req.word, req.sentence)

    # 5. Write Cache
    cur.execute(
        """
        INSERT INTO explain_cache
        (cache_key, word, sentence, meaning_zh, explanation_zh)
        VALUES (?, ?, ?, ?, ?)
        """,
        (cache_key, req.word, req.sentence, meaning, explanation)
    )
    conn.commit()
    conn.close()

    return {
        "word": req.word,
        "meaning_zh": meaning,
        "explanation_zh": explanation,
        "confidence": 0.95
    }

@app.post("/translate-text")
def translate_text(req: TranslateRequest):
    # ä½¿ç”¨åŠ¨æ€é…ç½®æˆ–é»˜è®¤é…ç½®åˆ›å»º AI æœåŠ¡
    service = create_ai_service(
        ai_provider=req.ai_provider,
        ai_api_key=req.ai_api_key,
        ai_base_url=req.ai_base_url,
        ai_model_name=req.ai_model_name,
        gemini_api_key=req.gemini_api_key,
        gemini_model_name=req.gemini_model_name
    )

    translation = service.translate_text(req.text)
    return {
        "translation_zh": translation
    }


# =========================
# AI é…ç½®ç®¡ç† API
# =========================

class AIConfigRequest(BaseModel):
    provider: str
    api_key: str = ""
    base_url: str = ""
    model_name: str = ""
    gemini_api_key: str = ""
    gemini_model_name: str = "gemini-1.5-flash"
    use_proxy: bool = False
    http_proxy: str = ""
    https_proxy: str = ""


@app.get("/api/config/providers")
def get_providers():
    """è·å–æ‰€æœ‰å¯ç”¨çš„ AI æä¾›å•†"""
    return {
        "providers": config_manager.get_all_providers()
    }


@app.get("/api/config/current")
def get_current_config():
    """è·å–å½“å‰é…ç½®"""
    return config_manager.get_current_config()


@app.post("/api/config/update")
def update_config(req: AIConfigRequest):
    """æ›´æ–° AI é…ç½®"""
    success = config_manager.update_config({
        "provider": req.provider,
        "api_key": req.api_key,
        "base_url": req.base_url,
        "model_name": req.model_name,
        "gemini_api_key": req.gemini_api_key,
        "gemini_model_name": req.gemini_model_name,
        "use_proxy": req.use_proxy,
        "http_proxy": req.http_proxy,
        "https_proxy": req.https_proxy
    })

    if success:
        return {
            "success": True,
            "message": "é…ç½®å·²æ›´æ–°ï¼Œè¯·é‡å¯åç«¯æœåŠ¡ä»¥åº”ç”¨æ–°é…ç½®"
        }
    else:
        raise HTTPException(status_code=500, detail="é…ç½®æ›´æ–°å¤±è´¥")


@app.post("/api/config/test")
def test_config(req: AIConfigRequest):
    """æµ‹è¯• AI é…ç½®æ˜¯å¦æœ‰æ•ˆ"""
    try:
        provider = req.provider

        if provider == "gemini":
            if not req.gemini_api_key:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘ Gemini API Key")

            from .ai_service import GeminiService
            test_service = GeminiService(
                api_key=req.gemini_api_key,
                model_name=req.gemini_model_name
            )
        else:
            if not req.api_key:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘ API Key")
            if not req.base_url:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘ Base URL")
            if not req.model_name:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘æ¨¡å‹åç§°")

            from .ai_service import AIService
            test_service = AIService(
                api_key=req.api_key,
                base_url=req.base_url,
                model_name=req.model_name
            )

        # æµ‹è¯•ç¿»è¯‘åŠŸèƒ½
        test_result = test_service.translate_text("Hello")

        return {
            "success": True,
            "message": "è¿æ¥æµ‹è¯•æˆåŠŸ",
            "test_result": test_result
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
        )
