from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
import os
import hashlib
import sqlite3
import spacy
from dotenv import load_dotenv
from google import genai
import pdfplumber
import docx
import io
import re
from google.genai import types
from fastapi import UploadFile, File, HTTPException

# =========================
# 1ï¸âƒ£ ç¯å¢ƒå˜é‡ & åŸºç¡€é…ç½®
# =========================

BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR / ".env"
load_dotenv(dotenv_path=ENV_PATH)

# ä»£ç†ï¼ˆå¦‚ä¸éœ€è¦å¯åˆ é™¤ï¼‰
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"

# Gemini API
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise RuntimeError("âŒ æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼Œè¯·æ£€æŸ¥ .env")

client = genai.Client(api_key=GEMINI_API_KEY)

# =========================
# 2ï¸âƒ£ FastAPI åˆå§‹åŒ–
# =========================

app = FastAPI(title="English Reader API")

from fastapi.staticfiles import StaticFiles
# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½• (ç”¨äº PDF è®¿é—®)
STATIC_DIR = BASE_DIR / "static"
if not STATIC_DIR.exists():
    STATIC_DIR.mkdir()

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# 3ï¸âƒ£ spaCy æ¨¡å‹
# =========================

nlp = spacy.load("en_core_web_sm")

# =========================
# 4ï¸âƒ£ SQLite ç¼“å­˜
# =========================

DB_PATH = BASE_DIR / "cache.db"

def get_conn():
    return sqlite3.connect(DB_PATH)

def init_cache():
    conn = get_conn()
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS explain_cache (
        cache_key TEXT PRIMARY KEY,
        word TEXT,
        sentence TEXT,
        meaning_zh TEXT,
        explanation_zh TEXT
    )
    """)
    conn.commit()
    conn.close()

init_cache()

def make_cache_key(sentence: str, word: str) -> str:
    h = hashlib.md5(sentence.strip().lower().encode()).hexdigest()[:8]
    return f"explain:{h}:{word.lower()}"

# =========================
# 5ï¸âƒ£ æ•°æ®æ¨¡å‹
# =========================

class ParseRequest(BaseModel):
    text: str

class ExplainRequest(BaseModel):
    token_id: str
    word: str
    sentence: str

# =========================
# 6ï¸âƒ£ Prompt æ„é€ 
# =========================

def build_prompt(word: str, sentence: str) -> str:
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­è¯­ä¹‰åˆ†æåŠ©æ‰‹ã€‚

è¯·ä»…æ ¹æ®ç»™å®šå¥å­ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œ
è§£é‡Šå•è¯ "{word}" åœ¨è¯¥å¥ä¸­çš„å…·ä½“å«ä¹‰ã€‚

å¥å­ï¼š
"{sentence}"

è¦æ±‚ï¼š
1. ç»™å‡ºå‡†ç¡®çš„ä¸­æ–‡é‡Šä¹‰ï¼ˆä¸è¶…è¿‡15ä¸ªå­—ï¼‰
2. ç”¨ä¸€å¥è¯è§£é‡Šè¯¥è¯åœ¨æ­¤å¤„çš„è¯­ä¹‰åŠŸèƒ½
3. ä¸è¦åˆ—å‡ºå…¶ä»–è¯ä¹‰
4. ä¸è¦ç¿»è¯‘æ•´ä¸ªå¥å­
"""

# =========================
# 6.5ï¸âƒ£ è¾…åŠ©ï¼šæ–‡æœ¬æ¸…æ´—ä¸è§£æ
# =========================

def clean_text(text: str) -> str:
    """æ¸…ç†æå–å‡ºçš„æ–‡æœ¬ï¼Œä½†ä¿ç•™æ®µè½ç»“æ„å’Œç¼©è¿›"""
    # 1. ç§»é™¤å¤šä½™çš„é¡µçœ‰é¡µè„š (ä»…è¿‡æ»¤æ‰çº¯æ•°å­—è¡Œï¼Œä½†ä¿ç•™ç©ºè¡Œä½œä¸ºæ®µè½æ ‡è®°)
    lines = text.splitlines()
    cleaned_lines = []
    
    for line in lines:
        # ä¸å† stripï¼Œä¿ç•™é¦–è¡Œç¼©è¿›
        if not line.strip(): 
            cleaned_lines.append("") # ä¿æŒç©ºè¡Œ
            continue
            
        if line.strip().isdigit(): continue # è¿‡æ»¤é¡µç 
        
        # ç§»é™¤è¡Œæœ«ç©ºæ ¼ï¼Œä½†ä¿ç•™è¡Œé¦–ç©ºæ ¼
        cleaned_lines.append(line.rstrip())
    
    text = "\n".join(cleaned_lines)

    # 2. å¤„ç†è¿å­—ç¬¦æ–­è¯ (è·¨è¡Œè¿å­—ç¬¦)
    text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
    
    # 3. è¿™é‡Œçš„æ–­è¡Œè¿æ¥é€»è¾‘éœ€è¦æ›´è°¨æ…ï¼Œä¸èƒ½æŠŠåˆ†æ®µç»™è¿ä¸Šäº†
    # åªæœ‰å½“ä¸‹ä¸€è¡Œä¸æ˜¯ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼Œæˆ–è€…æ˜¯å°å†™å­—æ¯å¼€å¤´æ—¶æ‰è¿æ¥ï¼Œä¸”ä¸èƒ½è·¨è¶Šç©ºè¡Œ
    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æš‚æ—¶ä¿ç•™æ‰€æœ‰æ¢è¡Œï¼Œäº¤ç”± process_text çš„ nlp å¤„ç†ï¼Œ
    # æˆ–è€…åªè¿æ¥é‚£äº›æ˜æ˜¾è¢«æˆªæ–­çš„å¥å­ã€‚
    # ä¸ºäº†ä¸¥æ ¼ä¿ç•™æ’ç‰ˆï¼Œæˆ‘ä»¬æš‚ä¸è¿›è¡Œæ¿€è¿›çš„æ–­è¡Œåˆå¹¶ï¼Œè€Œæ˜¯è®©å‰ç«¯è´Ÿè´£æ¸²æŸ“æ¢è¡Œã€‚
    # ä½† spaCy åˆ†å¥ä¾èµ–å®Œæ•´å¥å­ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†â€œéç©ºè¡Œä¹‹é—´çš„æ¢è¡Œâ€æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œ
    # è€Œä¿ç•™â€œç©ºè¡Œâ€ä½œä¸ºæ®µè½åˆ†éš”ç¬¦ã€‚
    
    # ç­–ç•¥è°ƒæ•´ï¼š
    # å°† \n\n+ æ›¿æ¢ä¸ºç‰¹æ®Šå ä½ç¬¦ <PARAGRAPH_BREAK>
    # å°† å•ä¸ª \n æ›¿æ¢ä¸ºç©ºæ ¼ (åˆå¹¶è¡Œ)
    # å†å°†å ä½ç¬¦è¿˜åŸ
    
    # ä¿æŠ¤æ®µè½
    text = re.sub(r'\n\s*\n', '<PARAGRAPH_BREAK>', text)
    
    # åˆå¹¶è¡Œ (éæ®µè½çš„æ¢è¡Œ -> ç©ºæ ¼)
    text = re.sub(r'\n', ' ', text)
    
    # è¿˜åŸæ®µè½ (è¿™é‡Œç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦è¡¨ç¤º)
    text = text.replace('<PARAGRAPH_BREAK>', '\n\n')
    
    # 4. å‹ç¼©ä¸­é—´çš„é‡å¤ç©ºæ ¼ (ä¿ç•™è¡Œé¦–çš„ç¼©è¿›æ¯”è¾ƒéš¾ï¼Œå› ä¸ºä¸Šé¢åˆå¹¶è¡Œæ—¶å·²ç»æ‰“ä¹±äº†)
    # ä¿®æ­£ç­–ç•¥ï¼šå¦‚æœç”¨æˆ·è¦æ±‚â€œä¸¥æ ¼ä¿ç•™ç¼©è¿›â€ï¼Œé‚£æˆ‘ä»¬ä¸èƒ½éšæ„åˆå¹¶è¡Œã€‚
    # ä½†å¦‚æœä¸åˆå¹¶è¡Œï¼ŒspaCy åˆ†å¥ä¼šå¾ˆçƒ‚ã€‚
    # è¿™æ˜¯ä¸€ä¸ªæƒè¡¡ã€‚ä¸ºäº† "è¯†åˆ«ç…§ç‰‡ä¸­çš„è‹±æ–‡æ–‡æœ¬å¹¶ä¸”æŒ‰ç…§ç…§ç‰‡ä¸­çš„æ’ç‰ˆæ ¼å¼"ï¼Œ
    # æˆ‘ä»¬å€¾å‘äºï¼šä»¥ç…§ç‰‡çš„è§†è§‰æ®µè½ä¸ºå‡†ã€‚
    
    # æ—¢ç„¶ä½¿ç”¨äº† Vision API (parse_image)ï¼Œå®ƒè¿”å›çš„é€šå¸¸å·²ç»æ˜¯å¾ˆå¥½çš„æ®µè½æ–‡æœ¬äº†ã€‚
    # å¯¹äº PDF (extract_words)ï¼Œæˆ‘ä»¬è‡ªå·±æ‹¼å‡‘çš„ text ä¹Ÿæœ‰æ®µè½æ¦‚å¿µ (\n\n)ã€‚
    
    # æ‰€ä»¥ï¼Œclean_text åªéœ€è¦è´Ÿè´£æ¸…ç†ä¹±ç ï¼Œä¸è¦ç ´å \n\n
    
    return text.strip()

def extract_words_with_coords(pdf_file):
    """
    æå– PDF æ–‡æœ¬åŠåæ ‡
    Returns:
        full_text: str
        text_map: list of dict, æ¯ä¸ªå…ƒç´ åŒ…å« {start: int, end: int, bbox: dict, page: int, page_width: float, page_height: float}
        pages_meta: list of dict, {page_idx: int, width: float, height: float}
    """
    full_text = ""
    text_map = []
    pages_meta = []
    
    current_char_idx = 0
    
    for page_idx, page in enumerate(pdf_file.pages):
        pages_meta.append({
            "page_idx": page_idx,
            "width": float(page.width),
            "height": float(page.height)
        })
        
        # extract_words è¿”å›: [{'text': 'foo', 'x0': ..., 'top': ...}, ...]
        words = page.extract_words(
            x_tolerance=1, 
            y_tolerance=1, 
            keep_blank_chars=False,
            use_text_flow=True
        )
        
        if not words: continue
        
        last_bottom = 0
        last_x1 = 0
        
        for i, w in enumerate(words):
            text = w['text']
            # åˆ¤æ–­æ˜¯å¦æ¢è¡Œ (top å˜åŒ–è¾ƒå¤§)
            if i > 0 and (w['top'] - words[i-1]['top']) > 5:
                full_text += "\n"
                current_char_idx += 1
                last_x1 = 0
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç©ºæ ¼ (x0 ä¸ä¸Šä¸€ä¸ª x1 çš„è·ç¦»)
            if last_x1 > 0 and (w['x0'] - last_x1) > 2: 
                full_text += " "
                current_char_idx += 1
            
            start = current_char_idx
            end = start + len(text)
            
            # è®°å½•æ˜ å°„
            text_map.append({
                "start": start,
                "end": end,
                "text": text,
                "bbox": {
                    "x0": float(w['x0']),
                    "top": float(w['top']),
                    "x1": float(w['x1']),
                    "bottom": float(w['bottom'])
                },
                "page": page_idx,
                "page_width": float(page.width),
                "page_height": float(page.height)
            })
            
            full_text += text
            current_char_idx = end
            
            last_x1 = w['x1']
            
        full_text += "\n\n"
        current_char_idx += 2
        
    return full_text, text_map, pages_meta

def parse_pdf(file_bytes: bytes):
    """
    æå– PDF æ–‡æœ¬ï¼Œè¿”å› (text, map, pages)
    å¦‚æœ extraction å¤±è´¥æˆ–å›é€€ï¼Œåˆ™ map ä¸º None
    """
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        # ä¼˜å…ˆå°è¯•åŸºäºå•è¯çš„é‡ç»„
        text, text_map, pages_meta = extract_words_with_coords(pdf)
        
        if len(text.strip()) > 50:
            return clean_text(text), text_map, pages_meta
        
        # å›é€€é€»è¾‘ (ä¸å¸¦åæ ‡)
        text = ""
        for page in pdf.pages:
            t = page.extract_text(x_tolerance=2, y_tolerance=2)
            if t: text += t + "\n"
        return clean_text(text), None, []

def ai_fix_text(text: str) -> str:
    """ä½¿ç”¨ Gemini ä¿®å¤æ’ç‰ˆæ··ä¹±çš„æ–‡æœ¬"""
    if len(text) < 100: return text # å¤ªçŸ­ä¸ä¿®
    
    # æˆªå–å‰ 2000 å­—ç¬¦åšä¸ªç¤ºä¾‹ï¼Œæˆ–è€…åˆ†æ®µä¿®ã€‚
    # ä¸ºèŠ‚çœ Token å’Œæ—¶é—´ï¼Œæˆ‘ä»¬åªä¿®æ•´æ˜æ˜¾çš„é—®é¢˜ï¼Œæˆ–è€…å…¨é‡ä¿®æ•´ï¼ˆè§†æ–‡æœ¬é•¿åº¦ï¼‰
    # è¿™é‡Œæ¼”ç¤ºå…¨é‡ä¿®æ•´ï¼Œå®é™…ç”Ÿäº§å¯èƒ½éœ€è¦åˆ†å—
    
    try:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ¸…æ´—åŠ©æ‰‹ã€‚æ”¶åˆ°çš„æ–‡æœ¬æ˜¯ä» PDF æå–çš„ï¼Œå¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
1. å•è¯ä¹‹é—´ç¼ºå°‘ç©ºæ ¼ (ä¾‹å¦‚ "hello,world" åº”ä¸º "hello, world")
2. å•è¯è¢«é”™è¯¯æˆªæ–­
3. åŒ…å«ä¹±ç æˆ–æ— æ„ä¹‰å­—ç¬¦

è¯·ä¿®å¤è¿™æ®µæ–‡æœ¬çš„æ ¼å¼ï¼Œä½¿å…¶è‡ªç„¶ã€æµç•…ã€å¯è¯»ã€‚
ä¿æŒåŸæœ‰å¥æ„ä¸å˜ã€‚åªè¾“å‡ºä¿®å¤åçš„æ–‡æœ¬ã€‚

å¾…ä¿®å¤æ–‡æœ¬ï¼š
{text[:4000]} 
""" 
# æ³¨æ„ï¼šè¿™é‡Œé™åˆ¶ 4000 å­—ç¬¦é˜²æ­¢æº¢å‡ºï¼Œå®é™…åº”åˆ†å—å¤„ç†ã€‚
# ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬æš‚ä¸”è¿™æ ·å¤„ç†ï¼Œæˆ–è€…ä»…ä¾èµ– clean_text 
        
        # è€ƒè™‘åˆ°æ€§èƒ½ï¼Œæˆ‘ä»¬å…ˆç”± clean_text å¤„ç†ï¼Œè¿™é‡Œä½œä¸ºå¯é€‰é¡¹
        # å®é™…ä»£ç ä¸­ï¼Œå¦‚æœç”¨æˆ·è§‰å¾— clean_text ä¸å¤Ÿï¼Œå¯ä»¥æ‰“å¼€è¿™ä¸ªå¼€å…³
        # ä¸ºå“åº”é€Ÿåº¦ï¼Œç›®å‰ä»…è¿”å› clean_text åçš„ç»“æœï¼Œè‹¥ç”¨æˆ·å¼ºæ±‚ AI ä¿®å¤ï¼Œå¯è§£å¼€ä¸‹æ–¹æ³¨é‡Š
        
        # response = client.models.generate_content(
        #     model="gemini-2.0-flash-lite",
        #     contents=prompt
        # )
        # return response.text.strip()
        pass 
    except:
        pass
    
    return clean_text(text)

def parse_image(file_bytes: bytes, mime_type: str) -> str:
    """ä½¿ç”¨ Gemini Vision è¯†åˆ«å›¾ç‰‡æ–‡æœ¬"""
    prompt = "Transcribe the text in this image, preserving the original layout and line breaks exactly. Do not add any conversational text."
    
    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash-lite",
            contents=[
                types.Part.from_bytes(data=file_bytes, mime_type=mime_type),
                prompt
            ]
        )
        return response.text.strip()
    except Exception as e:
        print(f"Vision API error: {e}")
        return ""

def parse_docx(file_bytes: bytes) -> str:
    """æå– Word æ–‡æœ¬"""
    doc = docx.Document(io.BytesIO(file_bytes))
    text = "\n".join([para.text for para in doc.paragraphs])
    return clean_text(text)

# =========================
# 7ï¸âƒ£ è·¯ç”±ï¼šè§£ææ–‡æœ¬
# =========================

@app.post("/upload-file")
async def upload_file(file: UploadFile = File(...)):
    """
    æ¥æ”¶ PDF/Word æ–‡ä»¶ -> è§£æä¸ºæ–‡æœ¬ -> è°ƒç”¨ NLP ç»“æ„åŒ–
    """
    content = await file.read()
    filename = file.filename.lower()
    
    text = ""
    
    text = ""
    word_map = None # ä»… PDF æœ‰
    pages_meta = []
    file_url = ""
    
    # ä¿å­˜æ–‡ä»¶åˆ°é™æ€ç›®å½•
    try:
        if filename.endswith(".pdf"):
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            safe_name = f"{hashlib.md5(content).hexdigest()[:10]}.pdf"
            save_path = STATIC_DIR / "uploads" / safe_name
            with open(save_path, "wb") as f:
                f.write(content)
            # URL (Assumes server runs on port 8000)
            file_url = f"http://127.0.0.1:8000/static/uploads/{safe_name}"

        if filename.endswith(".pdf"):
            text, word_map, pages_meta = parse_pdf(content)
        elif filename.endswith(".docx"):
            text = parse_docx(content)
        elif filename.endswith((".jpg", ".jpeg", ".png", ".webp")):
             text = parse_image(content, file.content_type or "image/jpeg")
        elif filename.endswith(".txt"):
            text = content.decode("utf-8")
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒ PDF, DOCX, TXT, JPG, PNG")
            
        if not text.strip():
             raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯†åˆ«")
        
        if word_map:
            # å¦‚æœæœ‰åæ ‡æ˜ å°„ï¼Œè·³è¿‡ AIä¿®å¤ï¼Œé˜²æ­¢å­—ç¬¦åç§»é”™ä¹±
            final_text = text
        else:
            final_text = ai_fix_text(text)
            
        result = process_text(final_text, word_map=word_map)
        
        # è¡¥å……æ–‡ä»¶ä¿¡æ¯
        result["file_url"] = file_url
        result["pages"] = pages_meta
        return result

    except Exception as e:
        print(f"è§£æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


def process_text(raw_text: str, word_map=None):
    """
    æ–‡æœ¬ç»“æ„åŒ–æ ¸å¿ƒé€»è¾‘
    :param word_map: list of dict, å­—ç¬¦ç´¢å¼•åˆ° PDF åæ ‡çš„æ˜ å°„
    """
    doc = nlp(raw_text)
    sentences = []
    
    # å°† generator è½¬æ¢ä¸º listï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦é€šè¿‡ç´¢å¼•è®¿é—®ä¸Šä¸€å¥
    all_sentences = list(doc.sents)

    for sent_idx, sent in enumerate(all_sentences):
        # 1. è®¡ç®—å¸ƒå±€ä¿¡æ¯
        # æ£€æŸ¥å¥å­åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®ä¹‹å‰ï¼Œæ˜¯å¦æœ‰æ¢è¡Œç¬¦
        # spaCy çš„ sent.start_char æŒ‡å‘å¥å­ç¬¬ä¸€ä¸ªå­—ç¬¦åœ¨ raw_text ä¸­çš„ä½ç½®
        
        layout_info = {
            "is_new_paragraph": False,
            "indent_level": 0
        }
        
        if sent_idx == 0:
            layout_info["is_new_paragraph"] = True
        else:
            # æŸ¥çœ‹ä¸Šä¸€å¥ç»“æŸåˆ°è¿™ä¸€å¥å¼€å§‹ä¹‹é—´çš„æ–‡æœ¬
            prev_sent_end = all_sentences[sent_idx-1].end_char if sent_idx > 0 else 0
            gap_text = raw_text[prev_sent_end : sent.start_char]
            
            # å¦‚æœä¸­é—´åŒ…å«è‡³å°‘ä¸¤ä¸ªæ¢è¡Œç¬¦ (æˆ–è€…æ ¹æ® clean_text çš„é€»è¾‘ï¼Œ\n\n)
            if '\n\n' in gap_text or gap_text.count('\n') >= 2:
                layout_info["is_new_paragraph"] = True
            
            # ç®€å•çš„ç¼©è¿›æ£€æµ‹ (æ£€æµ‹ gap_text æœ€åé¢çš„ç©ºæ ¼æ•°é‡ï¼Œæˆ–è€…å¥å­æœ¬èº«çš„ start_char å‰çš„ç©ºæ ¼)
            # ä½†ç”±äº clean_text å¯èƒ½ä¼šå‹ç¼©ç©ºæ ¼ï¼Œè¿™é‡Œä¸»è¦ä¾èµ– \n\n åˆ¤æ–­æ®µè½
        
        # ä½¿ç”¨æ‰‹åŠ¨åˆ—è¡¨è€Œä¸æ˜¯ç›´æ¥éå†ï¼Œæ–¹ä¾¿æ§åˆ¶åˆå¹¶
        spacy_tokens = list(sent)
        merged_tokens = []
        
        # éœ€è¦åˆå¹¶çš„åç¼€åˆ—è¡¨ (å…¨å°å†™)
        suffixes = ["n't", "'s", "'ll", "'re", "'ve", "'m", "'d"]

        for token in spacy_tokens:
            if token.is_space: continue
            
            should_merge = False
            if merged_tokens:
                text_lower = token.text.lower()
                # åªæœ‰å½“å®ƒæ˜¯ç‰¹å®šåç¼€ï¼Œä¸”ä¸æ˜¯å¥é¦–(è™½ç„¶æœ‰å‰ç½®tokené€šå¸¸ä¸æ˜¯å¥é¦–)æ—¶æ‰åˆå¹¶
                if text_lower in suffixes:
                     should_merge = True
            
            if should_merge:
                # åˆå¹¶åˆ°ä¸Šä¸€ä¸ª Token
                prev_token = merged_tokens[-1]
                prev_token['text'] += token.text
                prev_token['end'] = token.idx + len(token.text)
                # æ›´æ–°æ˜¯å¦æœ‰åç½®ç©ºæ ¼ï¼šç»§æ‰¿å½“å‰åç¼€çš„å±æ€§
                prev_token['has_space_after'] = bool(token.whitespace_)
            else:
                # æ–°å¢ Token
                token_data = {
                    "token_id": f"sent-{sent_idx}-token-{len(merged_tokens)}",
                    "text": token.text,
                    "lemma": token.lemma_,
                    "pos": token.pos_,
                    "tag": token.tag_,
                    "dep": token.dep_,
                    "start": token.idx,
                    "end": token.idx + len(token.text),
                    "has_space_after": bool(token.whitespace_)
                }
                
                # å¦‚æœæœ‰ word_mapï¼Œå°è¯•åŒ¹é…åæ ‡
                if word_map:
                    # æŸ¥æ‰¾ä¸å½“å‰ token (start, end) æœ‰é‡å çš„ PDF words
                    # è¿™é‡Œçš„ token.idx æ˜¯åœ¨ raw_text (å³ clean_text åçš„) ä¸­çš„ç´¢å¼•
                    # è€Œ word_map æ˜¯åœ¨ raw_text (clean_text å‰) ä¸­çš„ç´¢å¼•... 
                    # ğŸš¨ è­¦å‘Š: clean_text å¯èƒ½ä¼šæ”¹å˜ç´¢å¼• (ä¾‹å¦‚ clean_text æŠŠ \n æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œæˆ–è€…å»é™¤é¡µç )
                    # å¦‚æœ clean_text åšäº†å¤§å¹…ä¿®æ”¹ï¼Œç´¢å¼•å°±å¯¹ä¸ä¸Šäº†ã€‚
                    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åœ¨ PDF æ¨¡å¼ä¸‹ï¼Œå°½é‡è®© clean_text ä¸åšç ´åæ€§ä¿®æ”¹ï¼Œ
                    # æˆ–è€…ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ word_map åšåŒæ ·çš„ clean æ“ä½œã€‚
                    
                    # ç®€åŒ–ç­–ç•¥: ä»…ä»…å½“ clean_text æ²¡æœ‰å¤§è§„æ¨¡åˆ é™¤æ—¶æœ‰æ•ˆã€‚
                    # æ›´å¥½çš„ç­–ç•¥: è®© token åŒ¹é…å°½é‡å®½å®¹ï¼Œæˆ–è€…ä»…ä»…åŸºäºæ–‡æœ¬åŒ¹é… (ä¸å¯é )
                    # æš‚æ—¶æ–¹æ¡ˆ: å‡è®¾ clean_text åªæ˜¯ strip()ï¼Œå¦‚æœåšäº†æ›¿æ¢ï¼Œåç§»é‡ä¼šä¹±ã€‚
                    
                    # ä¿®æ­£: extract_words_with_coords æ„é€ å‡ºçš„ full_text å·²ç»æ˜¯ "æ¸…æ´" çš„ (é™¤äº†æœ€å strip)
                    # åªæœ‰ ai_fix_text ä¼šæä¹±å®ƒã€‚
                    # æ‰€ä»¥ï¼Œå¦‚æœæœ‰ word_mapï¼Œæˆ‘ä»¬åº”è¯¥è·³è¿‡ ai_fix_text æˆ–è€…æå…¶å°å¿ƒã€‚
                    
                    # å¯»æ‰¾é‡å 
                    matched_rects = []
                    t_start, t_end = token.idx, token.idx + len(token.text)
                    
                    for wm in word_map:
                        # æ£€æŸ¥åŒºé—´é‡å : max(start1, start2) < min(end1, end2)
                        if max(t_start, wm['start']) < min(t_end, wm['end']):
                            matched_rects.append(wm)
                            
                    if matched_rects:
                        # è®¡ç®— Union Box
                        page_idx = matched_rects[0]['page'] # å‡è®¾ token ä¸è·¨é¡µ
                        x0 = min(r['bbox']['x0'] for r in matched_rects)
                        top = min(r['bbox']['top'] for r in matched_rects)
                        x1 = max(r['bbox']['x1'] for r in matched_rects)
                        bottom = max(r['bbox']['bottom'] for r in matched_rects)
                        
                        token_data['bbox'] = {
                            "page": page_idx,
                            "x0": x0, 
                            "top": top,
                            "x1": x1,
                            "bottom": bottom,
                            "width": x1 - x0,
                            "height": bottom - top
                        }

                merged_tokens.append(token_data)
        
        # åªæœ‰å½“å¥å­åŒ…å«æœ‰æ•ˆ token æ—¶æ‰æ·»åŠ 
        if merged_tokens:
            sentences.append({
                "text": sent.text,
                "start": sent.start_char,
                "end": sent.end_char,
                "layout": layout_info,
                "tokens": merged_tokens
            })

    return {"sentences": sentences}

# =========================
# 7ï¸âƒ£ è·¯ç”±ï¼šè§£ææ–‡æœ¬
# =========================

@app.post("/parse-text")
def parse_text(req: ParseRequest):
    return process_text(req.text)

# =========================
# 8ï¸âƒ£ è·¯ç”±ï¼šä¸Šä¸‹æ–‡é‡Šä¹‰ï¼ˆå”¯ä¸€ç‰ˆæœ¬ï¼‰
# =========================

@app.post("/explain-token")
def explain_token(req: ExplainRequest):
    cache_key = make_cache_key(req.sentence, req.word)

    conn = get_conn()
    cur = conn.cursor()

    # â‘  æŸ¥ç¼“å­˜
    cur.execute(
        "SELECT meaning_zh, explanation_zh FROM explain_cache WHERE cache_key = ?",
        (cache_key,)
    )
    row = cur.fetchone()

    if row:
        conn.close()
        return {
            "word": req.word,
            "meaning_zh": row[0],
            "explanation_zh": row[1],
            "confidence": 0.95
        }

    # â‘¡ è°ƒç”¨ Gemini
    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash-lite",
            contents=build_prompt(req.word, req.sentence)
        )

        content = response.text.strip()
        lines = [l for l in content.splitlines() if l.strip()]

        meaning = lines[0]
        explanation = lines[1] if len(lines) > 1 else lines[0]

    except Exception as e:
        conn.close()
        return {
            "word": req.word,
            "meaning_zh": "æœåŠ¡é”™è¯¯",
            "explanation_zh": f"æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}",
            "confidence": 0.0
        }

    # â‘¢ å†™ç¼“å­˜
    cur.execute(
        """
        INSERT INTO explain_cache
        (cache_key, word, sentence, meaning_zh, explanation_zh)
        VALUES (?, ?, ?, ?, ?)
        """,
        (cache_key, req.word, req.sentence, meaning, explanation)
    )
    conn.commit()
    conn.close()

    return {
        "word": req.word,
        "meaning_zh": meaning,
        "explanation_zh": explanation,
        "confidence": 0.95
    }


#å¥å­ç¿»è¯‘
class TranslateRequest(BaseModel):
    text: str
def build_translate_prompt(text: str) -> str:
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è‹±è¯­ç¿»è¯‘åŠ©æ‰‹ã€‚

è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹å‡†ç¡®ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚

è¦æ±‚ï¼š
1. å¿ å®åŸæ„ï¼Œä¸è¦éšæ„æ‰©å±•
2. ä½¿ç”¨å­¦æœ¯/æ­£å¼ä¸­æ–‡è¡¨è¾¾
3. ä¸è¦æ·»åŠ è§£é‡Šæˆ–æ³¨é‡Š
4. åªè¾“å‡ºç¿»è¯‘ç»“æœ

è‹±æ–‡åŸæ–‡ï¼š
{text}
"""
@app.post("/translate-text")
def translate_text(req: TranslateRequest):
    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash-lite",
            contents=build_translate_prompt(req.text)
        )

        translation = response.text.strip()

        return {
            "translation_zh": translation
        }

    except Exception as e:
        return {
            "translation_zh": f"ç¿»è¯‘å¤±è´¥ï¼š{e}"
        }

